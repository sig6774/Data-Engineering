{
  "metadata": {
    "name": "1.readSource-scala",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val csvDF \u003d spark.read\n  .option(\"header\", \"true\")\n  .option(\"delimiter\", \",\")  \n  .option(\"inferSchema\",\"true\")\n  .csv(\"s3://sig-study-cloud/ods/danji_master.csv\")\n  \n  "
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvDF.show"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvDF.show"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvDF.createOrReplaceTempView(\"danji_master_csv\")\n// sql을 편하게 사용하기 위해 tempView 생성 "
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom danji_master_csv\n-- %sql은 sql 구문을 사용한다는 의미 "
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect sigungu, dong, count(*) as count\nfrom danji_master_csv\ngroup by sigungu, dong \norder by count(*) DESC"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvDF.coalesce(1)\n      .write\n      .mode(\"Overwrite\")\n      .format(\"json\")\n      .save(\"s3://sig-study-cloud/ods/test/danji_json/\")\n  \n // spark은 분산 처리환경이기 때문에 coalesce를 사용하지 않는다면 여러개의 파일이 생성 됨 \n // 특정 폴더 위치에 해당 파일을 저장 "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val jsonDF \u003d spark.read.json(\"s3://sig-study-cloud/ods/test/danji_json/part-00000-dd6c6d2d-b0ef-4245-8c2e-5ce66b824fb7-c000.json\")\n// 위에서 저장한 데이터(json)를 읽어서 dataframe으로 변환 "
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "jsonDF.show"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "jsonDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "jsonDF.createOrReplaceTempView(\"danji_master_json\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom danji_master_json"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nshow tables"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "jsonDF.write\n      .mode(\"Overwrite\")\n      .format(\"parquet\")\n      .save(\"s3://sig-study-cloud/ods/test/danji_parquet/\")\n// parquet 확장자로 데이터 저장"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val parquetDF \u003d spark.read.parquet(\"s3://sig-study-cloud/ods/test/danji_parquet/\")\n// 위에서 저장한 데이터를 spark의 dataframe으로 불러옴 \n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "parquetDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "parquetDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "parquetDF.createOrReplaceTempView(\"danji_master_pq\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom danji_master_pq"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "parquetDF.coalesce(1)\n      .write\n      .mode(\"Overwrite\")\n      .format(\"csv\")\n      .save(\"s3://sig-study-cloud/ods/test/danji_csv/\")\n// parquet로 불러온 데이터를 csv로 저장 "
    }
  ]
}