{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 20:52:31 WARN Utils: Your hostname, Moon-3.local resolves to a loopback address: 127.0.0.1; using 192.168.0.2 instead (on interface en0)\n",
      "23/03/26 20:52:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 20:52:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"learn-sql\").getOrCreate()\n",
    "# spark instance 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = [\n",
    "    ('Google', 'GOOGL', 'USA', 2984, 'USD'), \n",
    "    ('Netflix', 'NFLX', 'USA', 645, 'USD'),\n",
    "    ('Amazon', 'AMZN', 'USA', 3518, 'USD'),\n",
    "    ('Tesla', 'TSLA', 'USA', 1222, 'USD'),\n",
    "    ('Tencent', '0700', 'Hong Kong', 483, 'HKD'),\n",
    "    ('Toyota', '7203', 'Japan', 2006, 'JPY'),\n",
    "    ('Samsung', '005930', 'Korea', 70600, 'KRW'),\n",
    "    ('Kakao', '035720', 'Korea', 125000, 'KRW'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockSchema = [\"name\", \"ticket\", \"country\", \"price\", \"currency\"]\n",
    "# schema 생성, 데이터 타입을 작성해도 되고 안해도됨 \n",
    "# 만약 데이터 타입을 작성하지 않으면 spark에서 자동으로 데이터 타입을 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, ticket: string, country: string, price: bigint, currency: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=stocks, schema = stockSchema)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'),\n",
       " ('ticket', 'string'),\n",
       " ('country', 'string'),\n",
       " ('price', 'bigint'),\n",
       " ('currency', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "# spark에서 자동으로 데이터 타입을 인식한 것을 보여줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+------+--------+\n",
      "|   name|ticket|  country| price|currency|\n",
      "+-------+------+---------+------+--------+\n",
      "| Google| GOOGL|      USA|  2984|     USD|\n",
      "|Netflix|  NFLX|      USA|   645|     USD|\n",
      "| Amazon|  AMZN|      USA|  3518|     USD|\n",
      "|  Tesla|  TSLA|      USA|  1222|     USD|\n",
      "|Tencent|  0700|Hong Kong|   483|     HKD|\n",
      "| Toyota|  7203|    Japan|  2006|     JPY|\n",
      "|Samsung|005930|    Korea| 70600|     KRW|\n",
      "|  Kakao|035720|    Korea|125000|     KRW|\n",
      "+-------+------+---------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark sql에서 해당 df를 사용하기 위해 tempView를 생성\n",
    "df.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "| Google|\n",
      "|Netflix|\n",
      "| Amazon|\n",
      "|  Tesla|\n",
      "|Tencent|\n",
      "| Toyota|\n",
      "|Samsung|\n",
      "|  Kakao|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name from stocks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name| price|\n",
      "+-------+------+\n",
      "| Google|  2984|\n",
      "|Netflix|   645|\n",
      "| Amazon|  3518|\n",
      "|  Tesla|  1222|\n",
      "|Tencent|   483|\n",
      "| Toyota|  2006|\n",
      "|Samsung| 70600|\n",
      "|  Kakao|125000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, price from stocks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name| price|\n",
      "+-------+------+\n",
      "|Samsung| 70600|\n",
      "|  Kakao|125000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, price from stocks where country= 'Korea'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name| price|\n",
      "+-----+------+\n",
      "|Kakao|125000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, price from stocks where country= 'Korea' and price > 80000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|price|\n",
      "+-------+-----+\n",
      "| Google| 2984|\n",
      "|Netflix|  645|\n",
      "| Amazon| 3518|\n",
      "|  Tesla| 1222|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, price from stocks where country like 'U%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
